# Story 1.3: Scalable OpenAI API Endpoint

## Status
In Progress

## Story
**As a** developer,
**I want** to create a well-structured and documented backend endpoint that connects to OpenAI,
**so that** I have a scalable foundation for the AI agent.

## Acceptance Criteria
1. A /api/chat endpoint is created in the backend application.
2. The endpoint is well-documented using comments (e.g., JSDoc).
3. It takes a conversation history and proxies the request to the OpenAI API via the Vercel AI SDK.
4. The frontend sends its conversation to this endpoint and displays the streamed response.

## Tasks / Subtasks
- [x] Add `/api/chat` router with POST handler receiving `{ messages: Message[] }` (AC: 1)
  - [x] Validate request payload shape and return 400 on invalid input (AC: 1)
- [x] Integrate Vercel AI SDK client in backend service layer (AC: 3)
  - [x] Stream OpenAI responses back through Express response (AC: 3,4)
  - [x] Ensure API keys are resolved from environment variables with fallbacks for local dev (AC: 3)
- [x] Document endpoint with JSDoc and README snippet (AC: 2)
  - [x] Include request/response examples and error cases (AC: 2)
- [x] Update frontend to call `/api/chat` for assistant responses (AC: 4)
  - [x] Handle streaming updates in UI and reconcile with persistence logic (AC: 4)

## Dev Notes
- Reuse chat message interfaces defined during Story 1.2; keep them in a shared TypeScript module to avoid duplication.
- Follow `docs/architecture/tech-stack.md` guidance when configuring the Vercel AI SDK and ensure compatibility with Node 20 serverless runtime.
- Include exponential backoff or retry strategy for transient OpenAI errors to improve resilience.
- Keep logs structured; capture request IDs to aid debugging as specified in `docs/architecture/coding-standards.md`.

### Testing
- Add Jest tests covering payload validation and mocked OpenAI responses (use SDK mocks to simulate stream).
- Include integration test exercising `/api/chat` with supertest to confirm 200 responses and error branches.
- Execute manual end-to-end test from frontend to verify streamed content renders and persists correctly.

## Change Log
| Date       | Version | Description                 | Author |
| ---------- | ------- | --------------------------- | ------ |
| 2025-09-25 | 0.1     | Initial draft of story 1.3. | Codex  |

## Dev Agent Record

### Agent Model Used
- GPT-5 (Codex)

### Debug Log References
- `frontend: npm run lint` *(fails: Next.js could not download swc binary inside sandbox)*

### Completion Notes List
- Implemented `/api/chat` router with validation, structured logging, request-id propagation, and streaming support.
- Added chat service with retry/backoff around Vercel AI SDK; documented behaviour with JSDoc and backend README snippet.
- Wired frontend chat controller to stream assistant responses, update transcripts in real time, and persist final messages.
- Automated test coverage still pending; user deferred Jest alignment for later story pass.

### File List
- backend/src/api/chatRoutes.ts
- backend/src/services/chatService.ts
- backend/src/types/chat.types.ts
- backend/src/validation/chatValidation.ts
- backend/src/utils/logger.ts
- backend/README.md
- frontend/src/components/ChatController.tsx

## QA Results
